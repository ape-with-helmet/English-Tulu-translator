{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eCN72s1J9UJ",
        "outputId": "85f6925e-133e-4080-940b-80d7ff34ea7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             English  \\\n",
            "0                   The change was written by Chandu   \n",
            "1              God Gives Everything When He Gives It   \n",
            "2       Also, the pain of the mind does not go away.   \n",
            "3                       The potential is on the move   \n",
            "4  There was a science that we did not understand...   \n",
            "\n",
            "                                             Tulu  \n",
            "0                ಬದಲಾವಣೆ ಬರೆಯಿನಾರ್ ಚಂದು ಪನ್ಪಿನಾರ್  \n",
            "1                     ದೇವೆರ್ ಕೊರ್ನಗ ಮಾತಲ ಕೊರ್ಪೆರ್  \n",
            "2            ಅಂಚನೆ ಮನಸುಗು ಆಯಿನ ಬೇನೆ ಆ ಬೇನೆ ಮಾಜುಜಿ  \n",
            "3                               ವಿಭವೆ ನಡತೊಂದುಲ್ಲೆ  \n",
            "4  ನಮಕ್ ಸೊಲಬೊಗು ಅರ್ಥ ಆವಂದಿನ ಒಂಜಿ ವಿಜ್ಞಾನ ಇತ್ತ್ಂಡ್  \n",
            "X shape: (8300, 40)\n",
            "y_output shape: (8300, 39)\n",
            "X shape: (8300, 40)\n",
            "y_output shape: (8300, 40, 16949)\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, None, 1024)           7875584   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " multi_head_attention_16 (M  (None, None, 1024)           6715904   ['embedding_2[0][0]',         \n",
            " ultiHeadAttention)                                       0          'embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.add_32 (T  (None, None, 1024)           0         ['multi_head_attention_16[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " layer_normalization_32 (La  (None, None, 1024)           2048      ['tf.__operators__.add_32[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, None, 1024)           1049600   ['layer_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)        (None, None, 1024)           0         ['dense_18[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_33 (T  (None, None, 1024)           0         ['dropout_16[0][0]',          \n",
            " FOpLambda)                                                          'layer_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_33 (La  (None, None, 1024)           2048      ['tf.__operators__.add_33[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_17 (M  (None, None, 1024)           6715904   ['layer_normalization_33[0][0]\n",
            " ultiHeadAttention)                                       0         ',                            \n",
            "                                                                     'layer_normalization_33[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_34 (T  (None, None, 1024)           0         ['multi_head_attention_17[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'layer_normalization_33[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_34 (La  (None, None, 1024)           2048      ['tf.__operators__.add_34[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, None, 1024)           1049600   ['layer_normalization_34[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)        (None, None, 1024)           0         ['dense_19[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_35 (T  (None, None, 1024)           0         ['dropout_17[0][0]',          \n",
            " FOpLambda)                                                          'layer_normalization_34[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_35 (La  (None, None, 1024)           2048      ['tf.__operators__.add_35[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_18 (M  (None, None, 1024)           6715904   ['layer_normalization_35[0][0]\n",
            " ultiHeadAttention)                                       0         ',                            \n",
            "                                                                     'layer_normalization_35[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_36 (T  (None, None, 1024)           0         ['multi_head_attention_18[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'layer_normalization_35[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_36 (La  (None, None, 1024)           2048      ['tf.__operators__.add_36[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, None, 1024)           1049600   ['layer_normalization_36[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)        (None, None, 1024)           0         ['dense_20[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_37 (T  (None, None, 1024)           0         ['dropout_18[0][0]',          \n",
            " FOpLambda)                                                          'layer_normalization_36[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_37 (La  (None, None, 1024)           2048      ['tf.__operators__.add_37[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_19 (M  (None, None, 1024)           6715904   ['layer_normalization_37[0][0]\n",
            " ultiHeadAttention)                                       0         ',                            \n",
            "                                                                     'layer_normalization_37[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_38 (T  (None, None, 1024)           0         ['multi_head_attention_19[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'layer_normalization_37[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_38 (La  (None, None, 1024)           2048      ['tf.__operators__.add_38[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_21 (Dense)            (None, None, 1024)           1049600   ['layer_normalization_38[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)        (None, None, 1024)           0         ['dense_21[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_39 (T  (None, None, 1024)           0         ['dropout_19[0][0]',          \n",
            " FOpLambda)                                                          'layer_normalization_38[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_39 (La  (None, None, 1024)           2048      ['tf.__operators__.add_39[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_20 (M  (None, None, 1024)           6715904   ['layer_normalization_39[0][0]\n",
            " ultiHeadAttention)                                       0         ',                            \n",
            "                                                                     'layer_normalization_39[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_40 (T  (None, None, 1024)           0         ['multi_head_attention_20[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'layer_normalization_39[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_40 (La  (None, None, 1024)           2048      ['tf.__operators__.add_40[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, None, 1024)           1049600   ['layer_normalization_40[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)        (None, None, 1024)           0         ['dense_22[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_41 (T  (None, None, 1024)           0         ['dropout_20[0][0]',          \n",
            " FOpLambda)                                                          'layer_normalization_40[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_41 (La  (None, None, 1024)           2048      ['tf.__operators__.add_41[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_21 (M  (None, None, 1024)           6715904   ['layer_normalization_41[0][0]\n",
            " ultiHeadAttention)                                       0         ',                            \n",
            "                                                                     'layer_normalization_41[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_42 (T  (None, None, 1024)           0         ['multi_head_attention_21[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'layer_normalization_41[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_42 (La  (None, None, 1024)           2048      ['tf.__operators__.add_42[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_23 (Dense)            (None, None, 1024)           1049600   ['layer_normalization_42[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)        (None, None, 1024)           0         ['dense_23[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_43 (T  (None, None, 1024)           0         ['dropout_21[0][0]',          \n",
            " FOpLambda)                                                          'layer_normalization_42[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_43 (La  (None, None, 1024)           2048      ['tf.__operators__.add_43[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_22 (M  (None, None, 1024)           6715904   ['layer_normalization_43[0][0]\n",
            " ultiHeadAttention)                                       0         ',                            \n",
            "                                                                     'layer_normalization_43[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_44 (T  (None, None, 1024)           0         ['multi_head_attention_22[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'layer_normalization_43[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_44 (La  (None, None, 1024)           2048      ['tf.__operators__.add_44[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_24 (Dense)            (None, None, 1024)           1049600   ['layer_normalization_44[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)        (None, None, 1024)           0         ['dense_24[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_45 (T  (None, None, 1024)           0         ['dropout_22[0][0]',          \n",
            " FOpLambda)                                                          'layer_normalization_44[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_45 (La  (None, None, 1024)           2048      ['tf.__operators__.add_45[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_23 (M  (None, None, 1024)           6715904   ['layer_normalization_45[0][0]\n",
            " ultiHeadAttention)                                       0         ',                            \n",
            "                                                                     'layer_normalization_45[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_46 (T  (None, None, 1024)           0         ['multi_head_attention_23[0][0\n",
            " FOpLambda)                                                         ]',                           \n",
            "                                                                     'layer_normalization_45[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_46 (La  (None, None, 1024)           2048      ['tf.__operators__.add_46[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_25 (Dense)            (None, None, 1024)           1049600   ['layer_normalization_46[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)        (None, None, 1024)           0         ['dense_25[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_47 (T  (None, None, 1024)           0         ['dropout_23[0][0]',          \n",
            " FOpLambda)                                                          'layer_normalization_46[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_47 (La  (None, None, 1024)           2048      ['tf.__operators__.add_47[0][0\n",
            " yerNormalization)                                                  ]']                           \n",
            "                                                                                                  \n",
            " dense_26 (Dense)            (None, None, 16949)          1737272   ['layer_normalization_47[0][0]\n",
            "                                                          5         ']                            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 570950197 (2.13 GB)\n",
            "Trainable params: 570950197 (2.13 GB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            " 34/104 [========>.....................] - ETA: 14:29 - loss: 9.7645 - accuracy: 1.1489e-05"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib\n",
        "# matplotlib.use(\"TkAgg\")  # Or \"Qt5Agg\" if TkAgg is not available\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n",
        "\n",
        "data = pd.read_csv('translation_train_dataset.csv')\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())\n",
        "\n",
        "# Parameters\n",
        "max_length = 40  # Maximum length of sequences\n",
        "num_samples = len(data)  # Total number of samples\n",
        "\n",
        "# Initialize Tokenizer\n",
        "english_tokenizer = Tokenizer()\n",
        "tulu_tokenizer = Tokenizer()\n",
        "\n",
        "# Fit on English and Tulu sentences\n",
        "english_tokenizer.fit_on_texts(data['English'])\n",
        "tulu_tokenizer.fit_on_texts(data['Tulu'])\n",
        "\n",
        "# Save the tokenizers\n",
        "with open('english_tokenizer.pkl', 'wb') as file:\n",
        "    pickle.dump(english_tokenizer, file)\n",
        "\n",
        "with open('tulu_tokenizer.pkl', 'wb') as file:\n",
        "    pickle.dump(tulu_tokenizer, file)\n",
        "\n",
        "# Convert sentences to sequences\n",
        "english_sequences = english_tokenizer.texts_to_sequences(data['English'])\n",
        "tulu_sequences = tulu_tokenizer.texts_to_sequences(data['Tulu'])\n",
        "\n",
        "# Pad sequences\n",
        "english_sequences = pad_sequences(english_sequences, maxlen=max_length, padding='post')\n",
        "tulu_sequences = pad_sequences(tulu_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Prepare the input and output\n",
        "X = english_sequences\n",
        "y = tulu_sequences\n",
        "\n",
        "# Prepare the output by shifting\n",
        "y_input = y[:, :-1]  # Input sequences for the model\n",
        "y_output = y[:, 1:]  # Output sequences (shifted)\n",
        "\n",
        "# Before model fitting, print the shapes to debug\n",
        "print(f'X shape: {X.shape}')  # (num_samples, max_length)\n",
        "print(f'y_output shape: {y_output.shape}')  # (num_samples, max_length, num_classes)\n",
        "\n",
        "# Ensure the output sequences are padded to match the model's output shape\n",
        "y_output = pad_sequences(y_output, maxlen=max_length, padding='post')\n",
        "\n",
        "# One-hot encoding the output\n",
        "y_output = tf.keras.utils.to_categorical(y_output, num_classes=len(tulu_tokenizer.word_index) + 1)\n",
        "\n",
        "# Verify shapes\n",
        "print(f'X shape: {X.shape}')          # Should be (num_samples, max_length)\n",
        "print(f'y_output shape: {y_output.shape}')  # Should be (num_samples, max_length, num_classes)\n",
        "\n",
        "def create_transformer_model(input_dim, output_dim, embedding_dim=1024, num_heads=16, ff_dim=1024, dropout_rate = 0.2):\n",
        "    # Input layer\n",
        "    inputs = tf.keras.Input(shape=(None,))\n",
        "\n",
        "    # Embedding layer\n",
        "    x = tf.keras.layers.Embedding(input_dim=input_dim, output_dim=embedding_dim)(inputs)\n",
        "\n",
        "    # Transformer layers\n",
        "    for _ in range(8):\n",
        "        attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
        "        attn_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
        "\n",
        "        # Adjust Dense layer to match `embedding_dim`\n",
        "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(attn_output)\n",
        "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = tf.keras.layers.Dense(output_dim, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_transformer_model(len(english_tokenizer.word_index) + 1, len(tulu_tokenizer.word_index) + 1)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "initial_learning_rate = 1e-4\n",
        "decay_steps = 10000\n",
        "warmup_steps = 4000\n",
        "# Learning rate scheduler\n",
        "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    [warmup_steps], [initial_learning_rate / warmup_steps, initial_learning_rate])\n",
        "\n",
        "# Compile the model with the custom learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_translation_model.keras', save_best_only=True)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y_output,\n",
        "                    epochs=100,  # Increased epochs for better training\n",
        "                    batch_size=64,  # Common batch size\n",
        "                    validation_split=0.2,  # Use 20% of the data for validation\n",
        "                    callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the final model\n",
        "model.save('translation_model.keras')\n"
      ]
    }
  ]
}